"""Bedrock ãƒ­ã‚°ã‹ã‚‰ Prompt Caching ã®åŠ¹æœã‚’ç¢ºèªã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ."""

import boto3
import os
import json
import time
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()


def check_cache_metrics(
    model_id: str = "global.anthropic.claude-haiku-4-5-20251001-v1:0",
    hours: int = 1
):
    """Prompt Caching ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç¢ºèª.

    Args:
        model_id: ç¢ºèªã™ã‚‹ãƒ¢ãƒ‡ãƒ« ID
        hours: éå»ä½•æ™‚é–“åˆ†ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ã™ã‚‹ã‹
    """
    print("=" * 70)
    print("Prompt Caching ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèªï¼ˆBedrock ãƒ­ã‚°ã‹ã‚‰å–å¾—ï¼‰")
    print("=" * 70)
    print()

    region = os.getenv("AWS_REGION", "us-west-2")
    logs_client = boto3.client('logs', region_name=region)

    end_time = datetime.now(timezone.utc)
    start_time = end_time - timedelta(hours=hours)

    print(f"ğŸ“… æœŸé–“: {start_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
    print(f"      ï½ {end_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
    print(f"ğŸŒ ãƒªãƒ¼ã‚¸ãƒ§ãƒ³: {region}")
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model_id}")
    print()

    # CloudWatch Logs Insights ã‚¯ã‚¨ãƒª
    # ãƒ¢ãƒ‡ãƒ«IDã®ARNå½¢å¼ã¨IDå½¢å¼ã®ä¸¡æ–¹ã«å¯¾å¿œ
    # ARN: arn:aws:bedrock:region:account:inference-profile/global.anthropic.claude-haiku-4-5-20251001-v1:0
    query = f"""
    fields @timestamp, input.inputTokenCount, input.cacheReadInputTokenCount, input.cacheWriteInputTokenCount
    | filter modelId like /{model_id}/
    | stats
        sum(input.inputTokenCount) as totalInput,
        sum(input.cacheReadInputTokenCount) as totalCacheRead,
        sum(input.cacheWriteInputTokenCount) as totalCacheWrite,
        count(*) as requestCount
    """

    log_group = 'bedrock-logs'

    try:
        print("ğŸ” Bedrock ãƒ­ã‚°ã‚’æ¤œç´¢ä¸­...")

        # ã‚¯ã‚¨ãƒªã‚’é–‹å§‹
        start_query_response = logs_client.start_query(
            logGroupName=log_group,
            startTime=int(start_time.timestamp()),
            endTime=int(end_time.timestamp()),
            queryString=query
        )

        query_id = start_query_response['queryId']

        # ã‚¯ã‚¨ãƒªçµæœã‚’å¾…ã¤
        response = None
        max_wait = 30  # æœ€å¤§30ç§’å¾…ã¤
        elapsed = 0

        while elapsed < max_wait:
            time.sleep(1)
            elapsed += 1

            response = logs_client.get_query_results(queryId=query_id)

            if response['status'] == 'Complete':
                break
            elif response['status'] == 'Failed':
                print(f"âŒ ã‚¯ã‚¨ãƒªå¤±æ•—: {response.get('statistics', {})}")
                return
            elif response['status'] in ['Cancelled', 'Timeout']:
                print(f"âŒ ã‚¯ã‚¨ãƒªãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
                return

        if response['status'] != 'Complete':
            print(f"âš ï¸  ã‚¯ã‚¨ãƒªãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{max_wait}ç§’ï¼‰")
            return

        results_data = response.get('results', [])

        if not results_data or len(results_data) == 0:
            print("âš ï¸  å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: ãƒ‡ãƒ¼ã‚¿ãªã—")
            print("âš ï¸  ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿å–ã‚Š: ãƒ‡ãƒ¼ã‚¿ãªã—")
            print("âš ï¸  ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›¸ãè¾¼ã¿: ãƒ‡ãƒ¼ã‚¿ãªã—")
            print()
            print("-" * 70)

            # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®å‡¦ç†
            print("â„¹ï¸  æŒ‡å®šæœŸé–“ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            print()
            print("ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            print("  1. ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ:")
            print("     python experiments/prompt-caching/test_basic_caching.py")
            print()
            print("  2. æ•°åˆ†å¾…ã£ã¦ã‹ã‚‰å†åº¦ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ")
            print()
            print("=" * 70)
            return

        # çµæœã‚’è§£æ
        result_dict = {}
        for field in results_data[0]:
            result_dict[field['field']] = float(field['value']) if field['value'] else 0

        input_tokens = result_dict.get('totalInput', 0)
        cache_read = result_dict.get('totalCacheRead', 0)
        cache_write = result_dict.get('totalCacheWrite', 0)
        request_count = int(result_dict.get('requestCount', 0))

        print(f"âœ… å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: {input_tokens:,.0f} ãƒˆãƒ¼ã‚¯ãƒ³")
        print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿å–ã‚Š: {cache_read:,.0f} ãƒˆãƒ¼ã‚¯ãƒ³")
        print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›¸ãè¾¼ã¿: {cache_write:,.0f} ãƒˆãƒ¼ã‚¯ãƒ³")
        print(f"ğŸ“Š ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {request_count} ä»¶")
        print()
        print("-" * 70)

        results = {
            'InputTokens': input_tokens,
            'CacheReadInputTokens': cache_read,
            'CacheWriteInputTokens': cache_write,
        }

    except logs_client.exceptions.ResourceNotFoundException:
        print(f"âŒ ãƒ­ã‚°ã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {log_group}")
        print()
        print("ğŸ’¡ Bedrock ã®ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹æ–¹æ³•:")
        print()
        print("1. AWS ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§æœ‰åŠ¹åŒ–:")
        print("   - Amazon Bedrock ã‚³ãƒ³ã‚½ãƒ¼ãƒ« â†’ Settings â†’ Model invocation logging")
        print("   - CloudWatch Logs ã«ãƒ­ã‚°ã‚’é€ä¿¡ã™ã‚‹ã‚ˆã†ã«è¨­å®š")
        print(f"   - ãƒ­ã‚°ã‚°ãƒ«ãƒ¼ãƒ—å: {log_group}")
        print()
        print("2. AWS CLI ã§æœ‰åŠ¹åŒ–:")
        print("   aws bedrock put-model-invocation-logging-configuration \\")
        print("     --logging-config '{")
        print('       "cloudWatchConfig": {')
        print(f'         "logGroupName": "{log_group}",')
        print('         "roleArn": "arn:aws:iam::YOUR_ACCOUNT:role/BedrockLoggingRole"')
        print("       },")
        print('       "textDataDeliveryEnabled": true,')
        print('       "imageDataDeliveryEnabled": false,')
        print('       "embeddingDataDeliveryEnabled": false')
        print("     }'")
        print()
        print("3. ãƒ­ã‚°ãŒæœ‰åŠ¹åŒ–ã•ã‚Œã‚‹ã¾ã§æ•°åˆ†ã‹ã‹ã‚Šã¾ã™")
        print()
        print("=" * 70)
        return
    except Exception as e:
        print(f"âŒ ãƒ­ã‚°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        print()
        print("ğŸ’¡ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:")
        print("  - IAM æ¨©é™ã‚’ç¢ºèª: logs:StartQuery, logs:GetQueryResults")
        print(f"  - ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ãŒæ­£ã—ã„ã‹ç¢ºèª: AWS_REGION={region}")
        print()
        print("=" * 70)
        return

    # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
    total_input_tokens = input_tokens + cache_read + cache_write

    if total_input_tokens > 0:
        cache_hit_rate = (cache_read / total_input_tokens) * 100

        # ã‚³ã‚¹ãƒˆå‰Šæ¸›ç‡ã®è¨ˆç®—
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿å–ã‚Šåˆ†ã¯ 90% ã‚ªãƒ•
        cost_reduction = (cache_read * 0.9 / total_input_tokens) * 100

        print("ğŸ“Š çµ±è¨ˆæƒ…å ±")
        print("-" * 70)
        print(f"ç·å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_input_tokens:,.0f} ãƒˆãƒ¼ã‚¯ãƒ³")
        print(f"  - æ–°è¦å…¥åŠ›: {input_tokens:,.0f} ãƒˆãƒ¼ã‚¯ãƒ³")
        print(f"  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›¸ãè¾¼ã¿: {cache_write:,.0f} ãƒˆãƒ¼ã‚¯ãƒ³")
        print(f"  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿å–ã‚Š: {cache_read:,.0f} ãƒˆãƒ¼ã‚¯ãƒ³")
        print()
        print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {cache_hit_rate:.2f}%")
        print(f"ã‚³ã‚¹ãƒˆå‰Šæ¸›ç‡: ç´„ {cost_reduction:.1f}%")
        print()

        # è©³ç´°ãªã‚³ã‚¹ãƒˆè¨ˆç®—
        non_cached = input_tokens  # æ–°è¦å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ãªã„ï¼‰

        print("ğŸ’° ã‚³ã‚¹ãƒˆå†…è¨³ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç›¸å½“ï¼‰")
        print("-" * 70)
        print(f"æ–°è¦å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³:        {non_cached:>10,.0f} Ã— 1.00 = {non_cached:>10,.0f}")
        print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›¸ãè¾¼ã¿:      {cache_write:>10,.0f} Ã— 1.25 = {cache_write * 1.25:>10,.0f}")
        print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿å–ã‚Š:      {cache_read:>10,.0f} Ã— 0.10 = {cache_read * 0.10:>10,.0f}")
        print("-" * 70)

        total_cost = non_cached + (cache_write * 1.25) + (cache_read * 0.10)
        original_cost = total_input_tokens  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã®å ´åˆã®ç·ã‚³ã‚¹ãƒˆ
        saved = original_cost - total_cost

        print(f"å®Ÿè³ªã‚³ã‚¹ãƒˆ:              {total_cost:>10,.0f} ãƒˆãƒ¼ã‚¯ãƒ³ç›¸å½“")
        print(f"å…ƒã®ã‚³ã‚¹ãƒˆ:              {original_cost:>10,.0f} ãƒˆãƒ¼ã‚¯ãƒ³ç›¸å½“")
        print(f"å‰Šæ¸›é¡:                  {saved:>10,.0f} ãƒˆãƒ¼ã‚¯ãƒ³ ({saved/original_cost*100:.1f}%)")
        print()

        if cache_hit_rate > 50:
            print("ğŸ‰ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåŠ¹ã„ã¦ã„ã¾ã™ï¼")
            print("   2å›ç›®ä»¥é™ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å¤§å¹…ãªã‚³ã‚¹ãƒˆå‰Šæ¸›ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ”¹å–„ãŒ")
            print("   å®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚")
        elif cache_hit_rate > 0:
            print("âœ¨ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒéƒ¨åˆ†çš„ã«åŠ¹ã„ã¦ã„ã¾ã™")
            print("   ã•ã‚‰ã«åŠ¹æœã‚’é«˜ã‚ã‚‹ã«ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’1,024ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸Šã«")
            print("   ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        else:
            print("â„¹ï¸  ã¾ã ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            print("   åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›¸ãè¾¼ã¿ã®ã¿ãŒè¡Œã‚ã‚Œã¾ã™ã€‚")
            print("   2å›ç›®ä»¥é™ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆãŒç™ºç”Ÿã—ã¾ã™ã€‚")

    else:
        print("â„¹ï¸  æŒ‡å®šæœŸé–“ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
        print()
        print("ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("  1. ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ:")
        print("     python experiments/prompt-caching/test_basic_caching.py")
        print()
        print("  2. æ•°åˆ†å¾…ã£ã¦ã‹ã‚‰å†åº¦ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ")

    print()
    print("=" * 70)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Prompt Caching ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç¢ºèª")
    parser.add_argument(
        "--model-id",
        default="global.anthropic.claude-haiku-4-5-20251001-v1:0",
        help="ãƒ¢ãƒ‡ãƒ« IDï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Claude Haiku 4.5ï¼‰"
    )
    parser.add_argument(
        "--hours",
        type=int,
        default=1,
        help="éå»ä½•æ™‚é–“åˆ†ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰"
    )

    args = parser.parse_args()
    check_cache_metrics(model_id=args.model_id, hours=args.hours)
