"""DeepEval ã‚’ä½¿ç”¨ã—ãŸ Claude Agent SDK ã®è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ."""

from __future__ import annotations

import asyncio
import json
import os
from typing import List, Dict, Any, Union
from pathlib import Path

# DeepEval imports (ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã«æœ‰åŠ¹åŒ–)
try:
    from deepeval import evaluate
    from deepeval.test_case import LLMTestCase, LLMTestCaseParams
    from deepeval.metrics import (
        AnswerRelevancyMetric,
        FaithfulnessMetric,
        ContextualRelevancyMetric,
        HallucinationMetric,
        GEval,
    )
    DEEPEVAL_AVAILABLE = True
except ImportError:
    DEEPEVAL_AVAILABLE = False

try:
    from agent import BedrockAgentSDK
except ImportError:
    from src.agent import BedrockAgentSDK

from langfuse import get_client

# Bedrock Evaluator (ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ç”¨LLM)
EVALUATION_MODEL = None
EVALUATION_MODEL_NAME = "unknown"

try:
    try:
        from bedrock_evaluator import create_bedrock_evaluator, LANGCHAIN_AWS_AVAILABLE
    except ImportError:
        from src.bedrock_evaluator import create_bedrock_evaluator, LANGCHAIN_AWS_AVAILABLE

    if LANGCHAIN_AWS_AVAILABLE:
        # DeepEvalBaseLLM çµŒç”±ã§ Bedrock Haiku ã‚’ä½¿ç”¨
        EVALUATION_MODEL = create_bedrock_evaluator(
            model_id="anthropic.claude-3-haiku-20240307-v1:0",
        )
        EVALUATION_MODEL_NAME = "Bedrock Claude 3 Haiku (via DeepEvalBaseLLM)"
    else:
        # langchain-aws ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„
        EVALUATION_MODEL = "gpt-4"
        EVALUATION_MODEL_NAME = "OpenAI GPT-4 (langchain-aws not installed)"
except ImportError:
    # Bedrock evaluator ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ããªã„
    EVALUATION_MODEL = "gpt-4"
    EVALUATION_MODEL_NAME = "OpenAI GPT-4 (bedrock_evaluator.py not found)"

# Langfuse client
langfuse = get_client()


def load_evaluation_dataset(dataset_path: str) -> List[Dict[str, Any]]:
    """è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€.

    Args:
        dataset_path: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns:
        ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    with open(dataset_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    return data.get("test_cases", [])


def create_custom_metrics():
    """ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½œæˆ.

    Returns:
        ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    if not DEEPEVAL_AVAILABLE:
        return []

    # 1. ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã®æ­£ç¢ºæ€§
    tool_usage_metric = GEval(
        name="Tool Usage Correctness",
        criteria=(
            "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒé©åˆ‡ãªãƒ„ãƒ¼ãƒ«ã‚’é¸æŠã—ã€æ­£ã—ãå®Ÿè¡Œã—ãŸã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚"
            "è©•ä¾¡åŸºæº–:"
            "- ãƒ„ãƒ¼ãƒ«é¸æŠã®å¦¥å½“æ€§ï¼ˆã‚¿ã‚¹ã‚¯ã«é©ã—ãŸãƒ„ãƒ¼ãƒ«ã‹ï¼‰"
            "- ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã®æˆåŠŸï¼ˆã‚¨ãƒ©ãƒ¼ãªãå®Œäº†ã—ãŸã‹ï¼‰"
            "- æœŸå¾…ã•ã‚Œã‚‹çµæœã®ç”Ÿæˆï¼ˆæ„å›³ã—ãŸå‡ºåŠ›ãŒå¾—ã‚‰ã‚ŒãŸã‹ï¼‰"
        ),
        evaluation_params=[
            LLMTestCaseParams.INPUT,
            LLMTestCaseParams.ACTUAL_OUTPUT,
            LLMTestCaseParams.CONTEXT,
        ],
        threshold=0.8,
        model=EVALUATION_MODEL,  # Bedrock Haiku ã‚’ä½¿ç”¨
    )

    # 2. ãƒ¬ã‚¹ãƒãƒ³ã‚¹å“è³ª
    response_quality_metric = GEval(
        name="Response Quality",
        criteria=(
            "å›ç­”ã®ç·åˆçš„ãªå“è³ªã‚’è©•ä¾¡ã—ã¾ã™ã€‚"
            "è©•ä¾¡åŸºæº–:"
            "- æ˜ç¢ºæ€§: å›ç­”ã¯ç†è§£ã—ã‚„ã™ã„ã‹"
            "- ç°¡æ½”æ€§: å†—é•·ã§ãªã„ã‹"
            "- æœ‰ç”¨æ€§: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ã‹"
            "- æ­£ç¢ºæ€§: äº‹å®Ÿã«åŸºã¥ã„ã¦ã„ã‚‹ã‹"
        ),
        evaluation_params=[
            LLMTestCaseParams.INPUT,
            LLMTestCaseParams.ACTUAL_OUTPUT,
        ],
        threshold=0.75,
        model=EVALUATION_MODEL,  # Bedrock Haiku ã‚’ä½¿ç”¨
    )

    # 3. æ—¥æœ¬èªå“è³ª
    japanese_quality_metric = GEval(
        name="Japanese Language Quality",
        criteria=(
            "æ—¥æœ¬èªã®å“è³ªã‚’è©•ä¾¡ã—ã¾ã™ã€‚"
            "è©•ä¾¡åŸºæº–:"
            "- æ–‡æ³•ã®æ­£ç¢ºæ€§"
            "- è‡ªç„¶ãªè¡¨ç¾"
            "- æ•¬èªã®é©åˆ‡ãªä½¿ç”¨"
            "- æ–‡è„ˆã«å¿œã˜ãŸè¨€è‘‰é£ã„"
        ),
        evaluation_params=[
            LLMTestCaseParams.INPUT,
            LLMTestCaseParams.ACTUAL_OUTPUT,
        ],
        threshold=0.8,
        model=EVALUATION_MODEL,  # Bedrock Haiku ã‚’ä½¿ç”¨
    )

    return [
        tool_usage_metric,
        response_quality_metric,
        japanese_quality_metric,
    ]


def create_standard_metrics():
    """æ¨™æº–è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½œæˆ.

    Returns:
        æ¨™æº–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    if not DEEPEVAL_AVAILABLE:
        return []

    return [
        AnswerRelevancyMetric(
            threshold=0.7,
            model=EVALUATION_MODEL,  # Bedrock Haiku ã‚’ä½¿ç”¨
        ),
        FaithfulnessMetric(
            threshold=0.8,
            model=EVALUATION_MODEL,  # Bedrock Haiku ã‚’ä½¿ç”¨
        ),
        ContextualRelevancyMetric(
            threshold=0.7,
            model=EVALUATION_MODEL,  # Bedrock Haiku ã‚’ä½¿ç”¨
        ),
        HallucinationMetric(
            threshold=0.5,
            model=EVALUATION_MODEL,  # Bedrock Haiku ã‚’ä½¿ç”¨
        ),
    ]


async def run_agent_on_test_case(
    agent: BedrockAgentSDK,
    test_case: Dict[str, Any],
    index: int,
) -> tuple[Union[LLMTestCase, Dict[str, Any]], str]:
    """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã«å¯¾ã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ.

    Args:
        agent: Bedrock ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        test_case: ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
        index: ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

    Returns:
        (LLMTestCase ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ, trace_id)
    """
    print(f"  Running test case {index + 1}: {test_case['input'][:50]}...")

    # æ‰‹å‹•ã§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¦ trace_id ã‚’å–å¾—
    trace = langfuse.start_span(
        name=f"Evaluation Test Case {index + 1}",
        input=test_case["input"],
        metadata={
            "test_case_index": index,
            "expected_output": test_case.get("expected_output"),
        }
    )

    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ
    actual_output = await agent.chat(
        prompt=test_case["input"],
        session_id=f"eval-{index}",
        user_id="evaluator",
    )

    # ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’æ›´æ–°ã—ã¦çµ‚äº†
    trace.update(output=actual_output)
    trace.end()

    if not DEEPEVAL_AVAILABLE:
        # DeepEval ãŒåˆ©ç”¨ä¸å¯ã®å ´åˆã¯ãƒ€ãƒŸãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
        return {
            "input": test_case["input"],
            "actual_output": actual_output,
            "expected_output": test_case.get("expected_output"),
        }, trace.id

    # LLMTestCase ä½œæˆ
    test_case_obj = LLMTestCase(
        input=test_case["input"],
        actual_output=actual_output,
        expected_output=test_case.get("expected_output"),
        context=test_case.get("context", []),
        retrieval_context=test_case.get("retrieval_context", []),
    )

    return test_case_obj, trace.id


async def run_evaluation_simple(
    dataset_path: str = "datasets/evaluation_dataset.json",
    use_custom_metrics: bool = True,
):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªè©•ä¾¡ã‚’å®Ÿè¡Œ.

    Args:
        dataset_path: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹
        use_custom_metrics: ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    """
    if not DEEPEVAL_AVAILABLE:
        print("\n" + "=" * 60)
        print("âŒ DeepEval ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("=" * 60)
        print("\nä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š")
        print("  make eval-setup")
        print("\nã¾ãŸã¯æ‰‹å‹•ã§ï¼š")
        print("  uv pip install -e \".[evaluation]\"")
        print("\n" + "=" * 60)
        return

    print("=" * 60)
    print("DeepEval ã«ã‚ˆã‚‹ Claude Agent SDK è©•ä¾¡")
    print("=" * 60)
    print(f"\nğŸ¤– è©•ä¾¡ç”¨LLM: {EVALUATION_MODEL_NAME}")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    print(f"\nğŸ“ Loading dataset: {dataset_path}")
    test_data = load_evaluation_dataset(dataset_path)
    print(f"   Loaded {len(test_data)} test cases")

    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
    print("\nğŸ¤– Initializing agent...")
    agent = BedrockAgentSDK()

    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®Ÿè¡Œ
    print("\nğŸš€ Running test cases...")
    test_cases = []
    trace_ids = []
    for i, test_item in enumerate(test_data):
        test_case, trace_id = await run_agent_on_test_case(agent, test_item, i)
        test_cases.append(test_case)
        trace_ids.append(trace_id)
        break  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«1ã‚±ãƒ¼ã‚¹ã®ã¿å®Ÿè¡Œ

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æº–å‚™
    print("\nğŸ“Š Preparing metrics...")
    metrics = create_standard_metrics()

    if use_custom_metrics:
        custom_metrics = create_custom_metrics()
        metrics.extend(custom_metrics)
        print(f"   Using {len(metrics)} metrics ({len(custom_metrics)} custom)")
    else:
        print(f"   Using {len(metrics)} standard metrics")

    # è©•ä¾¡å®Ÿè¡Œ
    print("\nâš™ï¸  Evaluating...")
    results = evaluate(test_cases=test_cases, metrics=metrics)

    # çµæœã‚’ Langfuse ã«é€ä¿¡
    print("\nğŸ“¤ Sending evaluation scores to Langfuse...")
    test_results = getattr(results, 'test_results', [])

    scores_sent = 0
    for idx, test_result in enumerate(test_results):
        if idx >= len(trace_ids):
            break

        trace_id = trace_ids[idx]

        # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¹ã‚³ã‚¢ã‚’é€ä¿¡
        if hasattr(test_result, 'metrics_data'):
            for metric_data in test_result.metrics_data:
                langfuse.create_score(
                    name=metric_data.name,
                    value=metric_data.score,
                    trace_id=trace_id,
                    comment=metric_data.reason if hasattr(metric_data, 'reason') else None,
                )
                scores_sent += 1

    langfuse.flush()
    print(f"   Sent {scores_sent} scores to Langfuse across {len(test_results)} test cases")

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)

    print(f"ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: {len(test_cases)}")
    print(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°: {len(metrics)}")
    print(f"è©•ä¾¡ãƒ¢ãƒ‡ãƒ«: {EVALUATION_MODEL_NAME}")
    print(f"Langfuse ã‚¹ã‚³ã‚¢: {scores_sent} ä»¶")

    # è©³ç´°ãªçµæœã¯ DeepEval ã®ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™
    print("\nâœ… Evaluation completed!")
    print(f"ğŸ“Š è©³ç´°ãªçµæœã¯ä¸Šè¨˜ã® DeepEval å‡ºåŠ›ã‚’å‚ç…§ã—ã¦ãã ã•ã„")
    print(f"\nğŸ’¡ çµæœã®ç¢ºèªæ–¹æ³•:")
    print(f"  - DeepEval ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: deepeval view")
    print(f"  - Langfuse ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: https://cloud.langfuse.com")
    print(f"\nâœ¨ è©•ä¾¡ã‚¹ã‚³ã‚¢ã¯ Langfuse ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã«è¨˜éŒ²ã•ã‚Œã¾ã—ãŸï¼")


async def run_evaluation_with_report(
    dataset_path: str = "datasets/evaluation_dataset.json",
    output_path: str = "evaluation_report.json",
):
    """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä»˜ãã®è©•ä¾¡ã‚’å®Ÿè¡Œ.

    Args:
        dataset_path: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹
        output_path: ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ‘ã‚¹
    """
    await run_evaluation_simple(dataset_path)

    # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆï¼ˆå®Ÿè£…ã¯çœç•¥ï¼‰
    print(f"\nğŸ“„ Report saved to: {output_path}")


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°."""
    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    required_vars = [
        "LANGFUSE_SECRET_KEY",
        "LANGFUSE_PUBLIC_KEY",
        #"AWS_ACCESS_KEY_ID",     # Bedrock ç”¨
        #"AWS_SECRET_ACCESS_KEY",  # Bedrock ç”¨
    ]

    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        print(f"âŒ Missing environment variables: {', '.join(missing_vars)}")
        print("   Please set them in .env file")
        return

    # è©•ä¾¡å®Ÿè¡Œ
    await run_evaluation_simple(
        dataset_path="datasets/evaluation_dataset.json",
        use_custom_metrics=True,
    )


if __name__ == "__main__":
    asyncio.run(main())
