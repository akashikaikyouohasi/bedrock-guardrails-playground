# Langfuse ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å®Ÿè£…ã‚¬ã‚¤ãƒ‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€Claude Agent SDK ã¨ Bedrock ã‚’ä½¿ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ Langfuse ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ“– ç›®æ¬¡

- [ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](#ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—)
- [åŸºæœ¬çš„ãªå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³](#åŸºæœ¬çš„ãªå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³)
- [å®Ÿè£…ä¾‹](#å®Ÿè£…ä¾‹)
- [API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹](#api-ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹)
- [ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°](#ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°)
- [ãƒ†ã‚¹ãƒˆã¨ãƒ‡ãƒãƒƒã‚°](#ãƒ†ã‚¹ãƒˆã¨ãƒ‡ãƒãƒƒã‚°)

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# pyproject.toml ã«è¿½åŠ æ¸ˆã¿
uv pip install langfuse>=3.10.5 tiktoken>=0.5.0
```

### 2. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

`.env` ãƒ•ã‚¡ã‚¤ãƒ«ã« Langfuse ã®èªè¨¼æƒ…å ±ã‚’è¿½åŠ ï¼š

```bash
# Langfuse API Keys
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_HOST=https://cloud.langfuse.com  # ã‚ªãƒ—ã‚·ãƒ§ãƒ³

# AWS Credentials
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1

# Model Configuration
MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0
```

### 3. Langfuse ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–

```python
from langfuse import get_client

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹
langfuse = get_client()
```

## åŸºæœ¬çš„ãªå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

### ãƒ‘ã‚¿ãƒ¼ãƒ³ 1: æ‰‹å‹•ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰

**ä½¿ç”¨ã‚±ãƒ¼ã‚¹:** ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€è©³ç´°ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²

```python
async def chat_streaming(
    self,
    prompt: str,
    session_id: Optional[str] = None,
    user_id: Optional[str] = None,
) -> AsyncIterator[str]:
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    metadata = {
        "cwd": self.cwd,
        "aws_region": self.aws_region,
        "version": APP_VERSION,
        "streaming": "true",
        "sdk": "claude-agent-sdk",
    }
    if session_id:
        metadata["session_id"] = session_id
    if user_id:
        metadata["user_id"] = user_id

    # Generation é–‹å§‹
    generation = langfuse.start_generation(
        name="chat_streaming",
        model=self.model,
        input=prompt,
        model_parameters={
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        },
        metadata=metadata,
    )

    full_response = ""
    input_tokens = 0
    output_tokens = 0

    try:
        # ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®š
        input_tokens = estimate_tokens(prompt)

        # LLM å®Ÿè¡Œ
        async for message in query(prompt=prompt):
            message_text = extract_message_text(message)
            if message_text:
                full_response += message_text
                yield message_text

        # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®š
        output_tokens = estimate_tokens(full_response)

        # Generation æ›´æ–°
        generation.update(
            output=full_response,
            usage_details={
                "input": input_tokens,
                "output": output_tokens,
                "total": input_tokens + output_tokens,
            },
            metadata={
                "message_count": 1,
                "response_length": len(full_response),
            },
        )

    except Exception as e:
        generation.update(
            level="ERROR",
            status_message=str(e),
        )
        raise

    finally:
        # Generation çµ‚äº†
        generation.end()
        langfuse.flush()
```

### ãƒ‘ã‚¿ãƒ¼ãƒ³ 2: ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰

**ä½¿ç”¨ã‚±ãƒ¼ã‚¹:** ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…ã€éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°

```python
from langfuse import observe

@observe(as_type="generation")
async def chat(
    self,
    prompt: str,
    session_id: Optional[str] = None,
    user_id: Optional[str] = None,
) -> str:
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    metadata = {
        "cwd": self.cwd,
        "aws_region": self.aws_region,
        "version": APP_VERSION,
        "streaming": "false",
        "sdk": "claude-agent-sdk",
    }
    if session_id:
        metadata["session_id"] = session_id
    if user_id:
        metadata["user_id"] = user_id

    # Langfuse ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ›´æ–°
    langfuse.update_current_generation(
        model=self.model,
        input=prompt,
        model_parameters={
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        },
        metadata=metadata,
    )

    full_response = ""
    input_tokens = 0
    output_tokens = 0

    try:
        input_tokens = estimate_tokens(prompt)

        async for message in query(prompt=prompt):
            message_text = extract_message_text(message)
            if message_text:
                full_response += message_text + "\n"

        output_tokens = estimate_tokens(full_response)

    except Exception as e:
        langfuse.update_current_generation(
            level="ERROR",
            status_message=str(e),
        )
        raise

    # æœ€çµ‚æ›´æ–°
    langfuse.update_current_generation(
        output=full_response.strip(),
        usage_details={
            "input": input_tokens,
            "output": output_tokens,
            "total": input_tokens + output_tokens,
        },
        metadata={
            "message_count": 1,
            "response_length": len(full_response),
        },
    )

    return full_response.strip()
```

## å®Ÿè£…ä¾‹

### ä¾‹ 1: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ£ãƒƒãƒˆ

```python
from src.agent import BedrockAgentSDK

async def main():
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
    agent = BedrockAgentSDK(
        model="anthropic.claude-3-5-sonnet-20241022-v2:0",
        temperature=0.7,
        max_tokens=4096,
    )

    # ãƒãƒ£ãƒƒãƒˆå®Ÿè¡Œï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹è‡ªå‹•è¨˜éŒ²ï¼‰
    response = await agent.chat(
        prompt="é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        session_id="session-001",
        user_id="user-123",
    )

    print(response)
```

### ä¾‹ 2: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ

```python
from src.agent import BedrockAgentSDK

async def main():
    agent = BedrockAgentSDK()

    print("Assistant: ", end="", flush=True)

    async for chunk in agent.chat_streaming(
        prompt="AIã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„",
        session_id="session-002",
        user_id="user-456",
    ):
        print(chunk, end="", flush=True)

    print("\n")
```

### ä¾‹ 3: ãƒ„ãƒ¼ãƒ«ä½¿ç”¨

```python
from src.agent import BedrockAgentSDKWithClient

async def main():
    async with BedrockAgentSDKWithClient(
        tools=["Write", "Read"],
        temperature=0.5,
    ) as agent:
        async for chunk in agent.chat_with_client(
            prompt="hello.py ã‚’ä½œæˆã—ã¦ãã ã•ã„",
            session_id="session-003",
            user_id="user-789",
        ):
            print(chunk, end="", flush=True)
```

### ä¾‹ 4: ã‚·ãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªé–¢æ•°

```python
from src.agent import simple_query

async def main():
    response = await simple_query(
        prompt="2 + 2 = ?",
        session_id="session-004",
        user_id="user-101",
        temperature=0.0,  # æ±ºå®šè«–çš„ãªå›ç­”
    )
    print(response)
```

## API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

### `langfuse.start_generation()`

Generation ã‚’æ‰‹å‹•ã§é–‹å§‹ã—ã¾ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**

```python
generation = langfuse.start_generation(
    name: str,                              # å¿…é ˆ: Generation ã®åå‰
    model: str,                             # å¿…é ˆ: ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥å­
    input: Any,                             # å¿…é ˆ: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    output: Optional[Any] = None,           # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: å‡ºåŠ›ï¼ˆå¾Œã§ update å¯èƒ½ï¼‰
    metadata: Optional[Dict] = None,        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    model_parameters: Optional[Dict] = None,# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    usage_details: Optional[Dict] = None,   # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
    version: Optional[str] = None,          # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    level: Optional[str] = None,            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
    status_message: Optional[str] = None,   # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
)
```

**æˆ»ã‚Šå€¤:** `LangfuseGeneration` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

### `generation.update()`

Generation ã®æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**

```python
generation.update(
    output: Optional[Any] = None,           # å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆ
    usage_details: Optional[Dict] = None,   # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
    metadata: Optional[Dict] = None,        # è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    level: Optional[str] = None,            # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« ("ERROR" ãªã©)
    status_message: Optional[str] = None,   # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã©
)
```

### `generation.end()`

Generation ã‚’çµ‚äº†ã—ã¾ã™ã€‚

```python
generation.end()
```

### `langfuse.flush()`

ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã—ã¦ã€ã™ã¹ã¦ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ Langfuse ã«é€ä¿¡ã—ã¾ã™ã€‚

```python
langfuse.flush()
```

### `estimate_tokens()`

ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®šã—ã¾ã™ï¼ˆsrc/agent.pyï¼‰ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**

```python
tokens = estimate_tokens(
    text: str,                  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®šã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
    model: str = "gpt-4",       # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gpt-4ï¼‰
)
```

**æˆ»ã‚Šå€¤:** `int` - æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°

**å®Ÿè£…:**

```python
def estimate_tokens(text: str, model: str = "gpt-4") -> int:
    """Estimate token count for given text."""
    if not TIKTOKEN_AVAILABLE:
        # Fallback: rough estimation (1 token â‰ˆ 4 characters)
        return len(text) // 4

    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except Exception:
        # Fallback if model not found
        return len(text) // 4
```

## ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### åŸºæœ¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
generation = langfuse.start_generation(...)

try:
    # LLM å®Ÿè¡Œ
    response = await query(prompt)
    generation.update(output=response)

except Exception as e:
    # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¨˜éŒ²
    generation.update(
        level="ERROR",
        status_message=str(e),
    )
    raise

finally:
    # å¿…ãšçµ‚äº†å‡¦ç†
    generation.end()
    langfuse.flush()
```

### ç‰¹å®šã®ã‚¨ãƒ©ãƒ¼å‡¦ç†

```python
from botocore.exceptions import ClientError

try:
    response = await query(prompt)
    generation.update(output=response)

except ClientError as e:
    error_code = e.response['Error']['Code']
    generation.update(
        level="ERROR",
        status_message=f"AWS Error: {error_code}",
        metadata={"error_details": str(e)},
    )
    raise

except TimeoutError:
    generation.update(
        level="WARNING",
        status_message="Request timed out",
    )
    raise

finally:
    generation.end()
    langfuse.flush()
```

### ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯

```python
import asyncio

max_retries = 3
for attempt in range(max_retries):
    generation = langfuse.start_generation(
        name=f"chat_attempt_{attempt + 1}",
        model=self.model,
        input=prompt,
        metadata={"attempt": str(attempt + 1)},
    )

    try:
        response = await query(prompt)
        generation.update(output=response)
        generation.end()
        langfuse.flush()
        return response

    except Exception as e:
        generation.update(
            level="ERROR" if attempt == max_retries - 1 else "WARNING",
            status_message=f"Attempt {attempt + 1} failed: {str(e)}",
        )
        generation.end()
        langfuse.flush()

        if attempt < max_retries - 1:
            await asyncio.sleep(2 ** attempt)  # Exponential backoff
        else:
            raise
```

## ãƒ†ã‚¹ãƒˆã¨ãƒ‡ãƒãƒƒã‚°

### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆ

```python
# tests/test_tracing.py
import pytest
from src.agent import BedrockAgentSDK

@pytest.mark.asyncio
async def test_chat_with_tracing():
    """ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
    agent = BedrockAgentSDK()

    response = await agent.chat(
        prompt="ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        session_id="test-session",
        user_id="test-user",
    )

    assert response is not None
    assert len(response) > 0
```

### ãƒˆãƒ¬ãƒ¼ã‚¹ã®ç¢ºèª

Langfuse ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ç¢ºèªï¼š

1. https://cloud.langfuse.com ã«ãƒ­ã‚°ã‚¤ãƒ³
2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é¸æŠ
3. "Traces" ã‚¿ãƒ–ã‚’é–‹ã
4. session_id ã¾ãŸã¯ user_id ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

### ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰

```python
import logging

# Langfuse ã®ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–
logging.basicConfig(level=logging.DEBUG)
langfuse_logger = logging.getLogger("langfuse")
langfuse_logger.setLevel(logging.DEBUG)
```

### ãƒˆãƒ¬ãƒ¼ã‚¹ã®æ¤œè¨¼

```python
from langfuse import get_client

langfuse = get_client()

# ãƒˆãƒ¬ãƒ¼ã‚¹ãŒé€ä¿¡ã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
langfuse.flush()

# ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†å‰ã«å¿…ãš flush ã‚’å‘¼ã¶
import atexit
atexit.register(langfuse.flush)
```

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. å¸¸ã« flush ã‚’å‘¼ã¶

```python
try:
    # å‡¦ç†
    pass
finally:
    langfuse.flush()
```

### 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¸€è²«æ€§

```python
# å…±é€šã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å®šç¾©
def get_base_metadata():
    return {
        "aws_region": os.getenv("AWS_REGION"),
        "version": APP_VERSION,
        "environment": os.getenv("ENVIRONMENT", "development"),
    }

metadata = get_base_metadata()
metadata.update({"custom_field": "value"})
```

### 3. session_id ã®ç”Ÿæˆ

```python
import uuid

# ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã«ç”Ÿæˆ
session_id = str(uuid.uuid4())

# ä¼šè©±å…¨ä½“ã§åŒã˜ session_id ã‚’ä½¿ç”¨
for prompt in prompts:
    await agent.chat(
        prompt=prompt,
        session_id=session_id,
        user_id=user_id,
    )
```

### 4. ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®šã®ç²¾åº¦å‘ä¸Š

```python
# Claude ãƒ¢ãƒ‡ãƒ«ç”¨ã®æ¨å®šé–¢æ•°ï¼ˆå°†æ¥çš„ã«æ”¹å–„å¯èƒ½ï¼‰
def estimate_tokens_claude(text: str) -> int:
    # Claude å›ºæœ‰ã®æ¨å®šãƒ­ã‚¸ãƒƒã‚¯
    # ç¾åœ¨ã¯ GPT-4 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
    return estimate_tokens(text, model="gpt-4")
```

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

- [ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](./best-practices.md) - é‹ç”¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç¢ºèª
- [ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°æ¦‚è¦](./tracing-overview.md) - åŸºæœ¬æ¦‚å¿µã‚’å¾©ç¿’

## å‚è€ƒãƒªã‚½ãƒ¼ã‚¹

- [Langfuse Python SDK](https://langfuse.com/docs/observability/sdk/python/overview)
- [tiktoken](https://github.com/openai/tiktoken)
- [Claude Agent SDK](https://platform.claude.com/docs/en/agent-sdk/overview)
